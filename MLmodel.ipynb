import matplotlib.pyplot as plt
import numpy as np
import os
import re
import shutil
import string
import tensorflow as tf
import pandas as pd
import random
seed=42
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)
import tensorflow_hub as hub
import tensorflow_datasets as tfds
print(tf.__version__)
print(tf.executing_eagerly())
print(hub.__version__)
import seaborn as sns
# from tensorflow.keras import layers
# from tensorflow.keras import losses
train_ds,val_ds,test_ds=tfds.load(
    name='imdb_reviews',
    split=('train[:60%]','train[60%:]','test'),
    as_supervised=True
)
train_batch,label_batch=next(iter(train_ds.batch(10)))
label = [lab.numpy() for text, lab in val_ds]
from collections import Counter
print(Counter(label))
sns.countplot(x=label)
plt.title('Label distribution')
plt.show()
def Standarization(text):
  lowercase = tf.strings.lower(text)
  stripped_html = tf.strings.regex_replace(lowercase,'<br />',' ')
  return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation),'')

max_features = 10000
sequence_length = 150
vectorize_layer = tf.keras.layers.TextVectorization(
    standardize=Standarization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length
)
train_text = train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text.batch(512))
def vectorize_text(text,label):
  # text = tf.expand_dims(text,-1)
  return vectorize_layer(text),label

train_ds = train_ds.batch(32)
val_ds = val_ds.batch(32)
test_ds = test_ds.batch(32)

train_ds = train_ds.map(vectorize_text)
val_ds = val_ds.map(vectorize_text)
# test_ds = test_ds.map(vectorize_text)
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)
model = tf.keras.Sequential([
    # vectorize_layer,
    tf.keras.layers.Embedding(max_features,64),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64,activation='relu',kernel_regularizer='l2'),
    tf.keras.layers.Dense(1)
])

model.summary()
model.compile(
    optimizer='adam',
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])
callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=2,
    restore_best_weights=True
)
history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs=10,
    callbacks=[callback]
)
export_model = tf.keras.Sequential([
    vectorize_layer,
    model,
    tf.keras.layers.Activation('sigmoid')
])
export_model.compile(
    optimizer='adam',
    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),
    metrics = ['accuracy']
)
metrics = export_model.evaluate(test_ds, return_dict=True)
print(metrics)
ex1 = tf.constant([
    'The movie was so good! I love this movie , price worthit',
    'the movie is shit , does not worth to see the  movie money waste',
    'do not watch,bad not worth waste , pure waste',
    'waste of time and the money at the same time'
    ])
pre_array = export_model.predict(ex1)
j = 1
for i in pre_array:
  if i > 0.5 :
    print(f'{j}th Review is Positive.')
  else:
    print(f'{j}th Review is Negative.')
print(pre_array)
